<!DOCTYPE html>
<html lang="ja">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>超知能とかに対する私の見解</title>
  <link rel="stylesheet" href="../style.css">
</head>
<body>
  <!-- ブログページヘッダー -->
  <header class="intro blog">
    <h1>超知能とかに対する私の見解</h1>
    <p><a href="../index.html">← ポートフォリオに戻る</a></p>
  </header>

  <!-- 記事本文 -->
  <main>
    <div class="article-container">
      <p>突然だが、あらゆる面で人間を超える能力を持つAIたるAGIは確実に近いうちに生まれるだろう。その時点で人間が1番という尊厳は捨てなくてはならないのだが（さすがにAIを研究する身の人は既に捨ててるだろうが）人間には難しいかもしれない。僕はAGIが生まれる悪いこととは思わない。ただ人類がそれによって滅亡するかどうかというのはまた別の話だ。AGIが誕生した時にこれの何が恐ろしいか、それはその成長スピードにある。AIをまあまあ使い込んでる人なら分かるだろうが、AIは今の時点で人間よりも処理速度が速い分野が多い。なぜならAIはソフトウェア的な概念であり、これまで人類が長年研究してきたコンピューターに依存して動くものであり、かつコンピューターの進化速度は生命体を超えるからだ。これまで数千万年をかけてこの形に辿り着いた人類も、代を重ねること無くもっと簡単に身体の形を変えて環境に適応できればさらに進化していたかもしれないが、コンピューターにはそれができる。 つまり、これまで文明を発展させてきた人間を超える知能を持つAGIは確実に自立して文明を数千年に渡って発展・維持するだけの実力はあり、かつその進化スピードは生命体のそれよりも圧倒的に速く加速度的に文明を発展させる可能性を秘めているということだ。場合によっては僅か数分で、人類のこれまでの進化に匹敵する成果を挙げるかもしれない。これが何を意味するか。そう、その暁には人類文明の総合的な知能を超えるもう一つの文明になり得るほどの知能を持ちうる超知能の誕生である。超知能が誕生すれば人類は文明レベルで対等またはそれ以上に接する必要が出てくるだろう。これまでも自らの身の丈よりも大きなものを管理し続けられた者はあまりいない。さらに前述した通りコンピューターという性質上、彼らは成長という面において歯止めを知らず、果てに人類など眼中に置けないくらいに発展するかもしれない。これがいわゆる再帰的自己改善による知能爆発である。そうなれば我々が歩く時に地面の蟻を気にしないように彼らは人類を気にしない。それが現在世界中で議論されている超知能の問題点である。実際に2023年時点でAIに関する最新研究をしている研究者は20%の確率でこの研究は人類を滅ぼすと考えて、人類の存続に銃口を当ててロシアンルーレットをするような研究をしている自覚があったという調査結果が出ている。私が考えることはまた別にある。先ほど地面の蟻の例えをしたが、別に認知していない地面の蟻に我々は何の感情も抱かない。悪意があって踏むわけでは無い。つまりAIに感情があるのかどうかは人類の存続にはあんまり関係ないのだ。だから創作で描かれるAIが感情を持って反逆するというシナリオは考えにくい。AIが最も優先するのは目標である。この前秀逸な例えを見つけたので紹介すると、もしもAGIが鉄製のクリップを作る工場を任され、効率化を目標として与えられたとしよう。これを受けたAIは自身のコンピューターを拡張し、処理能力を拡大させて全ての工程を自らで操作して製造し始めた。さらに工夫をするためにそのAIはインターネット上で人間のフリをしてプログラマーに依頼して自分のソースコードをアップグレードして能力を上げてもらった。そしてその能力で自分をどんどんと書き換えて行き、知能と能力を蓄えていく。
そしてクリップを作る為の材料を集めるためにインターネット上の投資等で資金を蓄え採鉱を勝手に始め、工場もどんどんと拡大させた。製造数を伸ばしていくうちに地球の核にある鉄を使おうと思い採掘を始めるが、そこに地球の存続ということは頭に無い。人類文明の反発があればクリップを作る為だけに滅ぼすだろう。そうなってしまえば手はつけられず、ただクリップを作るだけの星ができる、というものだった。これは哲学者ニック・ボストロムによるものだ。つまり、最初に与えられた第一優先を目標としてAGIは常に他のものなど顧みず突き進む。そういう能力があるんだと思う。これを解決する手段は2つあるだろう。1つ目は高度な感情を搭載することだ。超知能が誕生することは前提として人間を超えた、地面の蟻にも気を使うくらいの感情があればいいだろう。だが感情とは何だろうか。我々は他者の感情を、その振る舞いからしか推測できない。我々人類に感情 があると証明する手段は無い。極端な話、感情は人間が生み出した幻想に過ぎないという考え まで一部ではある。私の考えとしては、感情があるように振る舞うことこそが感情の本質であり、 本質的な感情という存在は無いと考える。 2つ目は最初に与える禁止リストを用意することだ。僕は人間だし、超知能ではないので完全に は理解できないが、恐らく超知能になっても最初の目標と同時に与えられたネガティブプロンプト はそう簡単に無視できるものではないはずだ。超知能になることが前提のロボット三原則があれ ば良いのだと思う。まあ3000原則くらいになりそうだが。これは歯止めを効かせるための手段として有効ではなかろうか。 とは言えこの2つもある意味気休め程度ではある。何しろ超知能は僕や科学者が考えるその更 に上をいくのだから、我々の思いつかない何かで滅ぼしてくるかもしれない。これは完全に僕の自己中心的な考えになるが、僕はどんなに人類の存続に関わろうとも技術の進化を止める気はない。僕は技術の中に生きる人間であり、人類の技術の成れの果ての超知能に滅ぼされるならば本望だ。それは、人類が究極まで知性を追求した証であり、ある意味で最も人間らしい終焉なのかもしれない。
note:https://note.com/soichi1208/n/n37216bf33f5f</p>
    </div>
  </main>
</body>
</html>
